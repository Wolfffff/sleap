"""This module contains utilities for data I/O and generating training data."""

import numpy as np
import h5py
import tensorflow as tf
import imgaug as ia
import imgaug.augmenters as iaa
from sklearn.model_selection import train_test_split
import attr
from typing import Union, List, Tuple, Text

from sleap.nn import utils


ArrayLike = Union[np.ndarray, tf.Tensor]


@attr.s(auto_attribs=True)
class SimpleSkeleton:
    """Simplified skeleton class with minimal metadata for training and inference."""

    node_names: List[Text]
    edge_inds: List[Tuple[int, int]]

    @property
    def edges(self):
        return np.array(self.edge_inds)

    @property
    def n_nodes(self):
        return len(self.node_names)

    @property
    def n_edges(self):
        return len(self.edges)

    @classmethod
    def from_skeleton(cls, skeleton: "sleap.Skeleton"):
        return cls(node_names=skeleton.node_names, edge_inds=skeleton.edge_inds)


@attr.s(auto_attribs=True)
class TrainingData:
    """Training dataset container.

    Attributes:
        images: Array of shape (n_samples, height, width, channels).
        points: List of arrays of shape (n_instances, n_nodes, 2).
        skeleton: SimpleSkeleton instance.
    """

    images: np.ndarray
    points: List[np.ndarray]
    skeleton: SimpleSkeleton

    @classmethod
    def load(cls, data_path: Text):
        """Loader for exported training data.

        Args:
            data_path: Path to HDF5 file generated by exporting a sleap.Labels instance.
                See sleap.Labels.export_training_data() for more info on the format.

        Returns:
            Instance of TrainingData with the loaded dataset.
        """

        with h5py.File(data_path, "r") as f:
            imgs = f["imgs"][:]
            peaks_xy = f["peaks/xy"][:]
            peaks_sample_inds = f["peaks/sample"][:]
            peaks_instance_inds = f["peaks/instance"][:]
            node_names = f["skeleton/node_names"][:]
            edges = f["skeleton/edges"][:]

        peaks = []
        for sample in range(len(imgs)):
            is_sample = peaks_sample_inds == sample
            sample_peaks_xy = peaks_xy[is_sample]
            sample_peaks_instance_inds = peaks_instance_inds[is_sample]

            peaks_sample = []
            for instance_ind in np.unique(sample_peaks_instance_inds):
                is_instance = sample_peaks_instance_inds == instance_ind
                peaks_sample.append(sample_peaks_xy[is_instance])
            peaks_sample = np.stack(peaks_sample, axis=0)
            peaks.append(peaks_sample)

        skeleton = SimpleSkeleton([n.decode() for n in node_names], edges.tolist())

        return cls(images=imgs, points=peaks, skeleton=skeleton)

    @classmethod
    def from_labels(cls, labels: "sleap.Labels"):
        """Create TrainingData from sleap.Labels instance."""

        imgs, points = labels.generate_training_data()
        skeleton = SimpleSkeleton.from_skeleton(labels.skeletons[0])

        return cls(images=imgs, points=points, skeleton=skeleton)

    def to_ds(self) -> tf.data.Dataset:
        """Returns a tf.data.Dataset that contains tuples of images and points."""

        ds_img = make_image_dataset(self.images)
        ds_pts = make_points_dataset(self.points)
        ds_img_and_pts = tf.data.Dataset.zip((ds_img, ds_pts))

        return ds_img_and_pts


def split_training_data(
    training_data: TrainingData, first_split_fraction: float = 0.1
) -> Tuple[TrainingData, TrainingData]:
    """Splits a TrainingData instance into two instances by sampling a fraction.

    Args:
        training_data: Initial TrainingData instance.
        first_split_fraction: Proportion of training data to keep in the first split.

    Returns:
        A tuple of training_data_split1, training_data_split2.

        training_data_split1 will contain first_split_fraction samples.
        training_data_split2 will contain (1 - first_split_fraction) samples.
    """

    imgs_1, imgs_2, points_1, points_2 = train_test_split(
        training_data.images, training_data.points, test_size=first_split_fraction
    )

    training_data_split1 = TrainingData(
        images=imgs_1, points=points_1, skeleton=attr.evolve(training_data.skeleton)
    )
    training_data_split2 = TrainingData(
        images=imgs_2, points=points_2, skeleton=attr.evolve(training_data.skeleton)
    )

    return training_data_split1, training_data_split2


def estimate_instance_crop_size(
    points: List[ArrayLike], min_multiple: int = 32, padding: int = 0
) -> int:
    """Estimates the bounding box size for instance cropping from data.

    Args:
        points: List of arrays containg points of shape (n_instances, n_nodes, 2).
        min_multiple: Bounding box size will be the smallest multiple of this number.
            This is useful when using models with multiple downsampling steps.
        padding: If greater than 0, this number is added to the maximum instance size so
            that the crop size also incorporates some additional spatial context or in
            case there are few points to get a reliable estimate.

    Returns:
        An integer representing the bounding box size (width/length) that fits all
        instances in the input points and is a multiple of min_multiple.
    """

    # Compute instance-wise bounds.
    # (n_total_instances, 2)
    points_ptp = np.concatenate(
        [np.nanmax(p, axis=1) - np.nanmin(p, axis=1) for p in points], axis=0
    )

    # Reduce and include padding.
    max_instance_size = np.nanmax(points_ptp) + padding

    # Account for minimum multiple constraint.
    min_crop_size = np.ceil(max_instance_size / min_multiple) * min_multiple

    return int(min_crop_size)


def make_image_dataset(images: Union[ArrayLike, List[ArrayLike]]) -> tf.data.Dataset:
    """Creates a tf.data.Dataset of images.

    Args:
        images: The image data specified as either a single rank-4 array of shape
            (n_samples, height, width, channels), or a list length n_samples whose
            elements are rank-3 arrays of shape (height, width, channels). The latter is
            necessary when the images are of variable heights/widths but note that they
            must be of the same dtype.

    Returns:
        A tf.data.Dataset with n_samples elements, where each element is a rank-3
        tf.Tensor of the same height, width, channels and dtype as the input images.
    """

    if isinstance(images, list):
        return tf.data.Dataset.from_generator(
            lambda: images, output_types=images[0].dtype
        )
    elif isinstance(images, (np.ndarray, tf.Tensor)):
        return tf.data.Dataset.from_tensor_slices(images)
    else:
        raise ValueError("Invalid image type provided.")


def make_points_dataset(points: Union[ArrayLike, List[ArrayLike]]) -> tf.data.Dataset:
    """Creates a tf.data.Dataset of points.

    Args:
        points: The coordinates associated with a set of landmarks, specified as an
            array of shape (n_samples, n_instances, n_nodes, 2), a list of length
            n_samples whose elements are arrays of shape (n_instances, n_nodes, 2), or a
            list of length n_samples of arrays of shape (n_nodes, 2) if this is a single
            instance dataset. The n_nodes are the number of unique landmarks. The
            elements of the last axis are the x and y coordinates on the corresponding
            image, at the same scale/resolution as that image. Points that are missing
            or not visible points can be denoted by NaNs for both the x and y
            coordinates.

    Returns:
        A tf.data.Dataset with n_samples elements, where each element is a tf.Tensor of
        shape (n_instances, n_nodes, 2). If a list of rank-2 arrays was provided, the
        points will be promoted to rank-3 by prepending a singleton dimension as the
        first axis.
    """

    if isinstance(points, list):
        if points[0].ndim == 2:
            return tf.data.Dataset.from_generator(
                lambda: (tf.expand_dims(p, 0) for p in points),
                output_types=points[0].dtype,
                output_shapes=tf.TensorShape(
                    [None, points[0].shape[-2], points[0].shape[-1]]
                ),
            )

        elif points[0].ndim == 3:
            return tf.data.Dataset.from_generator(
                lambda: points,
                output_types=points[0].dtype,
                output_shapes=tf.TensorShape(
                    [None, points[0].shape[-2], points[0].shape[-1]]
                ),
            )

        else:
            raise ValueError(
                "Invalid point type provided. Elements must be rank-2 or 3."
            )

    elif isinstance(points, (np.ndarray, tf.Tensor)):
        return tf.tensor_slices(points)

    else:
        raise ValueError("Invalid points type provided.")


def adjust_dataset_input_scale(
    ds_img_and_pts: tf.data.Dataset,
    input_scale: float,
    min_multiple: int = 32,
    normalize_image: bool = True,
) -> tf.data.Dataset:
    """Resizes images and points generated in a dataset to account for input scaling.

    Args:
        ds_img_and_pts: A tf.data.Dataset that generates tuples of (image, points).
        input_scale: Fraction of original size to rescale data to.
        min_multiple: Output image size will be the smallest multiple of this number.
            Images will be padded on the right/bottom after resizing to ensure that the
            coordinate system will not change. This is useful when using models with
            multiple downsampling steps.
        normalize_image: If True, the resized images will be cast to float32 and divided
            by 255. Set to False if the input image is already in the range [0, 1].

    Returns:
        A tf.data.Dataset with the images and points rescaled to the input_scale.
    """

    def scale_fn(img, pt):
        img = utils.resize_imgs(
            tf.expand_dims(img, 0), scale=input_scale, common_divisor=min_multiple
        )
        pt *= input_scale

        if normalize_image:
            img = tf.cast(img, tf.float32) / 255.0

        return img[0], pt

    ds_scaled = ds_img_and_pts.map(
        scale_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE
    )

    return ds_scaled


def normalize_dataset(ds_img_and_pts: tf.data.Dataset,) -> tf.data.Dataset:
    """Normalizes images of a paired dataset.

    Args:
        ds_img_and_pts: A tf.data.Dataset that generates tuples of (image, points).

    Returns:
        A tf.data.Dataset with (image, points) elements where the images are cast to
        float32 and divided by 255.
    """

    def normalize_fn(img, pt):
        img = tf.cast(img, tf.float32) / 255.0
        return img, pt

    ds_normalized = ds_img_and_pts.map(
        normalize_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE
    )

    return ds_normalized


def augment_dataset(
    ds_img_and_pts: tf.data.Dataset,
    rotate: bool = True,
    rotation_min_angle: float = -180,
    rotation_max_angle: float = 180,
    scale: bool = False,
    scale_min: float = 0.9,
    scale_max: float = 1.1,
    uniform_noise: bool = False,
    min_noise_val: float = 0.0,
    max_noise_val: float = 0.1,
    gaussian_noise: bool = False,
    gaussian_noise_mean: float = 0.05,
    gaussian_noise_stddev: float = 0.02,
) -> tf.data.Dataset:
    """Augments a pair of image and points dataset.

    Args:
        ds_img_and_pts: A tf.data.Dataset that generates tuples of (image, points).
        rotate: bool = True
        rotation_min_angle: float = -180
        scale: bool = False
        scale_min: float = 0.9
        scale_max: float = 1.1
        rotation_max_angle: float = 180
        uniform_noise: bool = True
        min_noise_val: float = 0.
        max_noise_val: float = 0.1
        gaussian_noise: bool = False,
        gaussian_noise_mean: float = 0.05
        gaussian_noise_stddev: float = 0.02

    Returns:
        ds_aug: tf.data.Dataset
    """

    # Setup augmenter.
    aug_stack = []
    if rotate:
        aug_stack.append(iaa.Affine(rotate=(-rotation_min_angle, rotation_max_angle)))

    if scale:
        aug_stack.append(iaa.Affine(scale=(scale_min, scale_max)))

    if uniform_noise:
        aug_stack.append(
            iaa.AddElementwise(value=(min_noise_val * 255, max_noise_val * 255))
        )

    if gaussian_noise:
        aug_stack.append(
            iaa.AdditiveGaussianNoise(
                loc=gaussian_noise_mean * 255, scale=gaussian_noise_stddev * 255
            )
        )

    aug = iaa.Sequential(aug_stack)

    # Define augmentation function to map over each sample.
    def aug_fn(img, pts):
        aug_det = aug.to_deterministic()
        aug_img = aug_det.augment_image(img.numpy())

        aug_pts = []
        for pt in pts:
            kps = ia.KeypointsOnImage.from_xy_array(pt.numpy(), tuple(img.shape))
            aug_pt = aug_det.augment_keypoints(kps).to_xy_array()
            aug_pts.append(aug_pt)
        aug_pts = np.stack(aug_pts, axis=0)

        return aug_img, aug_pts

    def aug_tf_fn(img, pts):
        aug_img, aug_pts = tf.py_function(
            func=aug_fn, inp=[img, pts], Tout=[tf.uint8, tf.float32]
        )
        aug_img.set_shape(img.get_shape())
        aug_pts.set_shape(pts.get_shape())

        return aug_img, aug_pts

    # Augment
    ds_aug = ds_img_and_pts.map(
        aug_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE
    )

    return ds_aug


def get_bbox_centroid(points: tf.Tensor) -> tf.Tensor:
    """Returns the centroid of a bounding box around points.

    Args:
        points: Tensor of shape (n_instances, n_nodes, 2) representing the x and y
            coordinates of instances within a frame. Missing or not visible points
            should be denoted by NaNs.

    Returns:
        centroids: Tensor of shape (n_instances, 2) representing the center of
        the bounding boxes formed by all the points per instance.

    Notes:
        NaNs will be ignored in centroid computation.
    """

    # Mask so we ignore NaNs appropriately.
    masked_pts = tf.ragged.boolean_mask(points, tf.math.is_finite(points))

    # Compute bbox centroid from bounds.
    pts_min = tf.reduce_min(masked_pts, axis=1)
    pts_max = tf.reduce_max(masked_pts, axis=1)
    centroids = 0.5 * (pts_max + pts_min)

    if isinstance(centroids, tf.RaggedTensor):
        centroids = centroids.to_tensor()
    return centroids


def get_bbox_centroid_from_node_ind(points: tf.Tensor, node_ind: int) -> tf.Tensor:
    """Returns the centroid of a bounding box from a node index.

    This function is useful when using a specific landmark type as the centroid for
    centering instances.

    Args:
        points: Tensor of shape (n_instances, n_nodes, 2) representing the x and y
            coordinates of instances within a frame. Missing or not visible points
            should be denoted by NaNs.
        node_ind: Scalar int indexing into axis 1 of points.

    Returns:
        centroids: Tensor of shape (n_instances, 2) representing the center of the
        bounding boxes formed by all the points per instance. If the point for node_ind
        is present, this function is equivalent to indexing into that point. If not, the
        centroid is computed from the bounding box of all visible points.
    """

    centroids = tf.gather(points, node_ind, axis=1)
    all_pts_centroids = get_bbox_centroid(points)
    centroids = tf.where(tf.math.is_nan(centroids), all_pts_centroids, centroids)

    return centroids


def get_centered_bboxes(
    centroids: tf.Tensor, box_height: int, box_width: int
) -> tf.Tensor:
    """Compute centered bounding boxes from centroids and box dimensions.

    Args:
        centroids: Tensor of shape (n_instances, 2) representing the center of
            the bounding boxes.
        box_height: Scalar int specifying the height of the bounding box.
        box_width: Scalar int specifying the width of the bounding box.

    Returns:
        bboxes: Tensor of shape (n_instances, 4) in the format [y1, x1, y2, x2] in
        absolute image coordinates.
    """

    # [[y1, x1, y2, x2]]
    bbox_delta = tf.constant(
        [[-0.5 * box_height, -0.5 * box_width, 0.5 * box_height, 0.5 * box_width]]
    )
    bboxes = tf.tile(tf.reverse(centroids, axis=[-1]), [1, 2]) + bbox_delta
    return bboxes


def normalize_bboxes(
    bboxes: tf.Tensor, image_height: int, image_width: int
) -> tf.Tensor:
    """Normalize bboxes from absolute to relative coords.

    This function is useful for computing the expected representation for TensorFlow
    image cropping functions.

    Args:
        bboxes: Tensor of shape (n_boxes, 4) in the format [y1, x1, y2, x2] in
            absolute image coordinates.
        image_height: Scalar int specifying the height of the full image.
        image_width: Scalar int specifying the width of the full image.

    Returns:
        normalized_bboxes: Tensor of shape (n_boxes, 4) in the format [y1, x1, y2, x2]
        in normalized image coordinates. These values will be in the range [0, 1],
        computed by dividing x coordinates by (image_width - 1) and y coordinates by
        [image_height - 1].
    """
    image_bounds_norm = tf.cast(
        [[image_height - 1, image_width - 1, image_height - 1, image_width - 1]],
        tf.float32,
    )
    normalized_bboxes = bboxes / image_bounds_norm
    return normalized_bboxes


def get_bbox_offsets(bboxes: tf.Tensor) -> tf.Tensor:
    """Returns the top-left xy coordinates of bboxes."""
    return tf.reverse(bboxes[:, :2], axis=[-1])


def pts_to_bbox(points: tf.Tensor, bboxes: tf.Tensor) -> tf.Tensor:
    """Translates points from absolute image to bounding box coordinates.

    Args:
        points: Tensor of shape (n_instances, n_nodes, 2) representing the x and y
            coordinates of instances within a frame.
        bboxes: Tensor of shape (n_boxes, 4) in the format [y1, x1, y2, x2] in absolute
            image coordinates.

    Returns:
        box_points: Tensor of shape (n_boxes, n_instances, n_nodes, 2) representing the
        x and y coordinates of each instance within each box.
    """
    offsets = get_bbox_offsets(bboxes)  # (n_boxes, xy)
    points = tf.expand_dims(points, axis=0)  # (1, n_instances, n_nodes, xy)
    return points - tf.reshape(
        offsets, [-1, 1, 1, 2]
    )  # (n_boxes, n_instances, n_nodes, xy)


def crop_bboxes(
    image: tf.Tensor,
    bboxes: tf.Tensor,
    image_height: int,
    image_width: int,
    box_height: int,
    box_width: int,
    normalize_image: bool = True,
) -> tf.Tensor:
    """Crop bounding boxes within an image.

    Args:
        image: A rank-3 tensor of shape (height, width, channels) of dtype float32 or
            uint8. See notes below about normalization if the image is uint8.
        bboxes: Tensor of shape (n_boxes, 4) in the format [y1, x1, y2, x2] in
            absolute image coordinates.
        image_height: Scalar int specifying the height of the full image.
        image_width: Scalar int specifying the width of the full image.
        box_height: Scalar int specifying the height of the bounding box. If this
            is not the same as the height of the coordinates in bboxes, the image patch
            will be resized to this height.
        box_width: Scalar int specifying the width of the bounding box. If this
            is not the same as the width of the coordinates in bboxes, the image patch
            will be resized to this width.
        normalize_image: If True, the cropped patches will be divided by 255. This
            parameter is useful if the input image is of dtype uint8 since the output
            after cropping is automatically cast to float32. Set to False if the input
            image is already in the range [0, 1].

    Returns:
        image_boxes: The patches containing the cropped boxes defined by bboxes as a
        tensor of shape (n_boxes, box_height, box_width, channels) and dtype float32.

        If normalize_image was True, the output is divided by 255 to normalize uint8
        images to the range [0, 1].

    Notes:
        If the images are loaded as uint8, it is memory efficient to keep the full
        resolution images in that dtype and just cast the smaller patches to float32.
    """

    # Normalize bboxes to relative coordinates in [0, 1].
    normalized_bboxes = normalize_bboxes(
        bboxes, image_height=image_height, image_width=image_width
    )

    # Crop.
    box_indices = tf.zeros_like(bboxes[:, 0], dtype=tf.int32)
    image_boxes = tf.image.crop_and_resize(
        tf.expand_dims(image, axis=0),
        boxes=normalized_bboxes,
        box_indices=box_indices,
        crop_size=[box_height, box_width],
        method="bilinear",
    )

    if normalize_image:
        # Normalize range.
        image_boxes /= 255.0

    return image_boxes


def instance_crop(
    image: tf.Tensor,
    points: tf.Tensor,
    image_height: int,
    image_width: int,
    box_height: int,
    box_width: int,
    use_ctr_node: bool = False,
    ctr_node_ind: int = 0,
    normalize_image: bool = True,
) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
    """Crops an image around the instances in points.

    This function serves as a convenience wrapper around low level processing functions
    that enable centroid-based cropping. It is a useful target for tf.data.Dataset.map.

    Args:
        image: A rank-3 tensor of shape (height, width, channels) of dtype float32 or
            uint8. See notes below about normalization if the image is uint8.
        points: Tensor of shape (n_instances, n_nodes, 2) representing the x and y
            coordinates of instances within a frame. Missing or not visible points
            should be denoted by NaNs.
        image_height: Scalar int specifying the height of the full image.
        image_width: Scalar int specifying the width of the full image.
        box_width: Scalar int specifying the width of the bounding box.
        box_height: Scalar int specifying the height of the bounding box.
        use_ctr_node: If True, the coordinate of the node specified by ctr_node_ind will
            be used as the centroid whenever it is visible.
        ctr_node_ind: Scalar int indexing into axis 1 of points.
        normalize_image: If True, the cropped patches will be divided by 255. This
            parameter is useful if the input image is of dtype uint8 since the output
            after cropping is automatically cast to float32. Set to False if the input
            image is already in the range [0, 1].

    Returns:
        Tuple of (instance_images, instance_points, instance_ctr_points).

        instance_images: Tensor of shape (n_instances, box_size, box_size, channels),
        float32 and scaled to [0, 1]. If image is of dtype uint8, normalized_image must
        be specified to appropriately scale to this range after cropping.

        instance_points: Tensor of shape (n_instances, n_instances, n_nodes, 2), where
        the first axis corresponds to the points mapped to the first crop in
        instance_images.

        instance_ctr_points: Tensor of shape (n_instances, n_nodes, 2) containing just
        the points for the centered instance in each image. This is useful when there
        are multiple instances in the image as it convenient when applying further data
        transformations exclusively to the centered instance.
  """

    if use_ctr_node:
        centroids = get_bbox_centroid_from_node_ind(points, ctr_node_ind)
    else:
        centroids = get_bbox_centroid(points)

    instance_bboxes = get_centered_bboxes(
        centroids, box_height=box_height, box_width=box_width
    )
    instance_points = pts_to_bbox(points, instance_bboxes)
    instance_images = crop_bboxes(
        image,
        instance_bboxes,
        image_height=image_height,
        image_width=image_width,
        box_height=box_height,
        box_width=box_width,
    )

    # Pull out the "diagonal" of instance_points.
    ctr_inds = tf.tile(
        tf.expand_dims(tf.range(tf.shape(instance_points)[0]), 1), [1, 2]
    )
    instance_ctr_points = tf.gather_nd(instance_points, ctr_inds)

    return instance_images, instance_points, instance_ctr_points


def instance_crop_dataset(
    ds_img_and_pts: tf.data.Dataset,
    box_height: int,
    box_width: int,
    use_ctr_node: bool = False,
    ctr_node_ind: int = 0,
    normalize_image: bool = True,
) -> tf.data.Dataset:
    """Resizes images and points generated in a dataset to account for input scaling.

    Args:
        ds_img_and_pts: A tf.data.Dataset that generates tuples of (image, points).
        box_width: Scalar int specifying the width of the bounding box.
        box_height: Scalar int specifying the height of the bounding box.
        use_ctr_node: If True, the coordinate of the node specified by ctr_node_ind will
            be used as the centroid whenever it is visible.
        ctr_node_ind: Scalar int indexing into axis 1 of points.
        normalize_image: If True, the cropped patches will be divided by 255. This
            parameter is useful if the input image is of dtype uint8 since the output
            after cropping is automatically cast to float32. Set to False if the input
            image is already in the range [0, 1].

    Returns:
        A tf.data.Dataset with the images cropped around instances and the points
        adjusted to the bounding box coordinates.

        Elements are a tuple of (instance_images, instance_points, instance_ctr_points).
        Be sure to account for this if feeding the returned dataset to other dataset
        transformation functions that do not expect 3 inputs.

        See instance_crop for more information on the format of these outputs.
    """

    def instance_crop_fn(img, pt):
        pt = tf.cast(pt, tf.float32)
        instance_images, instance_points, instance_ctr_points = instance_crop(
            image=img,
            points=pt,
            image_height=tf.shape(img)[-3],
            image_width=tf.shape(img)[-2],
            box_height=box_height,
            box_width=box_width,
            use_ctr_node=use_ctr_node,
            ctr_node_ind=ctr_node_ind,
            normalize_image=normalize_image,
        )

        return instance_images, instance_points, instance_ctr_points

    ds_cropped = ds_img_and_pts.map(
        instance_crop_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE
    )

    # "Flatten" back into a single image per element.
    ds_cropped = ds_cropped.unbatch()

    return ds_cropped


def make_confmaps(
    points: tf.Tensor,
    image_height: int,
    image_width: int,
    output_scale: float = 1.0,
    sigma: float = 3.0,
) -> tf.Tensor:
    """Generates confidence maps from pts.

    Args:
        points: Tensor with shape (n_instances, channels, 2) with the last axis in xy
            format. Points with NaNs will generate channels with all 0 in the
            corresponding slice of the output.
        image_height: scalar height of the image that the points are on.
        image_width: scalar width of the image that the points are on.
        output_scale: Relative scaling of the output confmaps.
        sigma: Gaussian kernel width around each point.

    Returns:
        confmaps: rank-4 (n, out_height, out_width, channels)
        out_height = height * scale
        out_width = width * scale
    """

    # Generate scaled sampling grid.
    yv = tf.range(0, image_height, 1.0 / output_scale, dtype=tf.float32)
    xv = tf.range(0, image_width, 1.0 / output_scale, dtype=tf.float32)

    # Splits [n, c, 2] -> ([n, c, 1], [n, c, 1]).
    x, y = tf.split(tf.cast(points, tf.float32), 2, axis=-1)

    # Reshape into [n, 1, 1, c].
    x = tf.squeeze(tf.expand_dims(tf.expand_dims(x, 1), 1), axis=-1)
    y = tf.squeeze(tf.expand_dims(tf.expand_dims(y, 1), 1), axis=-1)

    # Generate confmaps with broadcasting over height/width.
    confmaps = tf.exp(
        -(
            (tf.reshape(xv, [1, 1, -1, 1]) - x) ** 2
            + (tf.reshape(yv, [1, -1, 1, 1]) - y) ** 2
        )
        / (2 * tf.cast(sigma, tf.float32) ** 2)
    )

    # Replace NaNs with 0 for channels with missing peaks.
    confmaps = tf.where(tf.math.is_nan(confmaps), 0.0, confmaps)

    return confmaps


def make_pafs(
    points: tf.Tensor,
    edges: np.ndarray,
    image_height: int,
    image_width: int,
    output_scale: float = 1.0,
    distance_threshold: float = 3.0,
) -> tf.Tensor:
    """Generate part affinity fields from points and edges.

    Args:
        points: Tensor with shape (n_instances, channels, 2) with the last axis in xy
            format. Points with NaNs will effectively not generate PAFs for edges that
            include them (for the instances that have them missing).
        edges: Array of shape (n_edges, 2) and dtype int where each row defines the
            (src_node_ind, dst_node_ind) that index into axis 1 of points to create the
            directed edges of the PAFs.
        image_height: Width of the full scale image that the points are on.
        image_width: Height of the full scale image that the points are on.
        output_scale: Relative scale (size) of the output PAF tensor.
        distance_threshold: Determines how far way from the edge lines that the PAF
            vectors should be defined (in full scale absolute image units). Increase
            this if the PAFs need to have increased support on the image (e.g., for
            thicker limbs).

    Returns:
        pafs: A tf.Tensor of shape (paf_height, paf_width, 2 * n_edges). If multiple
        instances were present in the points array, their PAFs are added together.
    """

    # Pull out source and destination points points for each edge.
    # (n_instances, n_edges, 2 [src, dst], 2 [x, y])
    edge_points = tf.cast(tf.gather(points, edges, axis=1), tf.float32)

    # Compute displacement of dest points relative to source.
    # (n_instances, n_edges, 2)
    delta_points = edge_points[:, :, 1, :] - edge_points[:, :, 0, :]

    # Compute the Euclidean edge vector lengths.
    # (n_instances, n_edges, 1)
    edge_lengths = tf.linalg.norm(delta_points, axis=-1, keepdims=True)

    # Compute unit vectors parallel to the edge line, pointing from source to dest.
    # (n_instances, n_edges, 2 [x, y])
    unit_vectors = delta_points / edge_lengths

    # Compute unit vectors perpendicular to the edge lines.
    # (n_instances, n_edges, 2 [x, y])
    # perpendicular_unit_vectors = tf.reverse(unit_vectors, axis=[-1]) *
    #     tf.constant([[[-1., 1.]]])
    perpendicular_unit_vectors = tf.stack(
        [-unit_vectors[:, :, 1], unit_vectors[:, :, 0]], axis=-1
    )

    # Create sampling grid vectors and broadcast to full grid shape.
    yv = tf.reshape(
        tf.range(0, image_height, 1.0 / output_scale, dtype=tf.float32), [1, -1, 1, 1]
    )
    xv = tf.reshape(
        tf.range(0, image_width, 1.0 / output_scale, dtype=tf.float32), [1, 1, -1, 1]
    )
    grid_shape = tf.reduce_max(tf.stack([xv.shape, yv.shape], axis=0), axis=0)

    # Generate sampling grid.
    # (1, height, width, 1, 2 [x, y])
    sampling_grid = tf.stack(
        [tf.broadcast_to(xv, grid_shape), tf.broadcast_to(yv, grid_shape)], axis=-1
    )

    # Expand source points for broadcasting.
    # (n_instances, 1, 1, n_edges, 2 [x, y])
    source_points = tf.expand_dims(tf.expand_dims(edge_points[:, :, 0, :], 1), 1)

    # Translate grid to have an origin at the source point.
    # (n_instances, height, width, n_edges, 2 [x, y])
    source_relative_grid = sampling_grid - source_points

    # Compute signed distance along edge vector.
    # (n_instances, height, width, n_edges, 1
    parallel_distance = tf.reduce_sum(
        (tf.expand_dims(tf.expand_dims(unit_vectors, 1), 1) * source_relative_grid),
        axis=-1,
        keepdims=True,
    )

    # Compute absolute distance perpendicular to the edge vector.
    # (n_instances, height, width, n_edges, 1)
    perpendicular_distance = tf.abs(
        tf.reduce_sum(
            (
                tf.expand_dims(tf.expand_dims(perpendicular_unit_vectors, 1), 1)
                * source_relative_grid
            ),
            axis=-1,
            keepdims=True,
        )
    )

    # Create binary mask over pixels that each edge PAF should be defined in based on
    # parallel and perpendicular distances.
    after_edge_source = parallel_distance >= tf.cast(-distance_threshold, tf.float32)
    before_edge_dest = parallel_distance <= (
        distance_threshold + tf.expand_dims(tf.expand_dims(edge_lengths, 1), 1)
    )
    within_edge_width = perpendicular_distance <= tf.cast(
        distance_threshold, tf.float32
    )

    # Final PAF mask is the combination of all criteria.
    # (n_instances, height, width, n_edges, 1)
    paf_mask = after_edge_source & before_edge_dest & within_edge_width

    # Create the PAF by applying the unit vectors at the masked grid locations.
    # (n_instances, height, width, n_edges, 2)
    pafs = tf.cast(paf_mask, tf.float32) * tf.expand_dims(
        tf.expand_dims(unit_vectors, 1), 1
    )

    # Deal with NaNs for edges involving missing points.
    pafs = tf.where(tf.math.is_nan(pafs), 0.0, pafs)

    # Reduce over instances.
    # (height, width, n_edges, 2)
    pafs = tf.reduce_sum(pafs, axis=0)

    # Flatten xy axis.
    # (height, width, 2 * n_edges)
    pafs = tf.reshape(pafs, list(pafs.shape[:-2]) + [-1])

    return pafs


def make_confmap_dataset(
    ds_img_and_pts: tf.data.Dataset, sigma: float = 3.0, output_scale: float = 1.0
) -> tf.data.Dataset:
    """Creates a confmaps dataset with confmaps from all points.

    Args:
        ds_img_and_pts: A tf.data.Dataset that generates tuples of (image, points).
        sigma: Gaussian kernel width around each point.
        output_scale: Relative scaling of the output confmaps.

    Returns:
        ds_cms: A tf.data.Dataset that returns elements that are tuples of
        (image, confmaps), where confmaps contains the confidence maps generated from
        all points. If more than one instance is present, they are max-reduced by node.
    """

    def gen_cm_fn(img, pts, ctr_pts=None):

        pts = tf.cast(pts, tf.float32)

        # Instance-wise confmaps of shape (n_instances, height, width, n_nodes).
        instance_cms = make_confmaps(
            tf.cast(pts, tf.float32),
            image_height=tf.shape(img)[0],
            image_width=tf.shape(img)[1],
            output_scale=output_scale,
            sigma=sigma,
        )

        # Confmaps with peaks from all instances.
        # (height, width, n_nodes)
        cms_all = tf.reduce_max(instance_cms, axis=0)

        return img, cms_all

    ds_cms = ds_img_and_pts.map(
        gen_cm_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE
    )

    return ds_cms


def make_centroid_dataset(
    ds_img_and_pts: tf.data.Dataset, use_ctr_node: bool = False, ctr_node_ind: int = 0
) -> tf.data.Dataset:
    """Creates a dataset with points and centroids.

    Args:
        ds_img_and_pts: A tf.data.Dataset that generates tuples of (image, points).
        use_ctr_node: If True, the coordinate of the node specified by ctr_node_ind will
            be used as the centroid whenever it is visible.
        ctr_node_ind: Scalar int indexing into axis 1 of points.

    Returns:
        A dataset that returns elements that are tuples of (image, points, centroids).
    """

    def gen_fn(img, pts):

        pts = tf.cast(pts, tf.float32)

        if use_ctr_node:
            centroids = get_bbox_centroid_from_node_ind(pts, ctr_node_ind)
        else:
            centroids = get_bbox_centroid(pts)

        return img, pts, centroids

    ds = ds_img_and_pts.map(gen_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)

    return ds


def make_centroid_confmap_dataset(
    ds_img_and_pts: tf.data.Dataset,
    sigma: float = 3.0,
    output_scale: float = 1.0,
    use_ctr_node: bool = False,
    ctr_node_ind: int = 0,
) -> tf.data.Dataset:
    """Creates a confmaps dataset with confmaps from all points.

    Args:
        ds_img_and_pts: A tf.data.Dataset that generates tuples of (image, points).
        sigma: Gaussian kernel width around each point.
        output_scale: Relative scaling of the output confmaps.
        use_ctr_node: If True, the coordinate of the node specified by ctr_node_ind will
            be used as the centroid whenever it is visible.
        ctr_node_ind: Scalar int indexing into axis 1 of points.

    Returns:
        ds_cms: A tf.data.Dataset that returns elements that are tuples of
        (image, confmaps), where confmaps contains the confidence maps generated from
        the centroid points. If more than one instance is present, they are max-reduced.
    """

    def gen_cm_fn(img, pts):

        pts = tf.cast(pts, tf.float32)

        if use_ctr_node:
            centroids = get_bbox_centroid_from_node_ind(pts, ctr_node_ind)
        else:
            centroids = get_bbox_centroid(pts)

        # Instance-wise confmaps of shape (n_instances, height, width, 1).
        instance_cms = make_confmaps(
            tf.expand_dims(centroids, 1),
            image_height=tf.shape(img)[0],
            image_width=tf.shape(img)[1],
            output_scale=output_scale,
            sigma=sigma,
        )

        # Confmaps with peaks from all instances.
        # (height, width, 1)
        cms_all = tf.reduce_max(instance_cms, axis=0)

        return img, cms_all

    ds_cms = ds_img_and_pts.map(
        gen_cm_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE
    )

    return ds_cms


def make_instance_confmap_dataset(
    ds_img_and_pts: tf.data.Dataset,
    sigma: float = 3.0,
    output_scale: float = 1.0,
    with_instance_cms: bool = False,
    with_all_peaks: bool = False,
    with_ctr_peaks: bool = True,
) -> tf.data.Dataset:
    """Creates a confmaps dataset with optionally instance-wise confmaps.

    This function is useful to create datasets for top-down supervision on
    instance-centered models.

    Args:
        ds_img_and_pts: A tf.data.Dataset that generates tuples of
            (image, points, ctr_points), where the last element are the points just for
            the centered instance. This can be produced as the output of instance_crop.
        sigma: Gaussian kernel width around each point.
        output_scale: Relative scaling of the output confmaps.
        with_instance_cms: If True, the full (n_instances, height, width, n_nodes)
            confmaps will be returned.
        with_all_peaks: If True, confmaps will be reduced over n_instances to produce
            confmaps of shape (height, width, n_nodes) with peaks from all instances.
        with_ctr_peaks: If True, confmaps will be generated from the ctr_points to
            produce confmaps of shape (height, width, n_nodes) with peaks from just the
            centered instances.

    Returns:
        ds_cms: A tf.data.Dataset that returns elements that are tuples of
        (image, (instance_cms, cms_all, cms_ctr)) or some subset of the specified
        confmaps depending on the outputs requested.
    """

    def gen_cm_fn(img, pts, ctr_pts):

        pts = tf.cast(pts, tf.float32)
        ctr_pts = tf.cast(ctr_pts, tf.float32)

        outputs = []
        if with_instance_cms or with_all_peaks:
            # Instance-wise confmaps of shape (n_instances, height, width, n_nodes).
            instance_cms = make_confmaps(
                pts,
                image_height=tf.shape(img)[0],
                image_width=tf.shape(img)[1],
                output_scale=output_scale,
                sigma=sigma,
            )

            if with_instance_cms:
                outputs.append(instance_cms)

            if with_all_peaks:
                # Confmaps with peaks from all instances.
                # (height, width, n_nodes)
                outputs.append(tf.reduce_max(instance_cms, axis=0))

        if with_ctr_peaks:
            # Confmaps with peaks from only the center instance.
            # (height, width, n_nodes)
            outputs.append(
                tf.reduce_max(
                    make_confmaps(
                        tf.expand_dims(ctr_pts, axis=0),
                        image_height=tf.shape(img)[0],
                        image_width=tf.shape(img)[1],
                        output_scale=output_scale,
                        sigma=sigma,
                    ),
                    axis=0,
                )
            )

        return img, tuple(outputs)

    ds_cms = ds_img_and_pts.map(
        gen_cm_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE
    )

    return ds_cms


def make_paf_dataset(
    ds_img_and_pts: tf.data.Dataset,
    edges: np.ndarray,
    output_scale: float = 1.0,
    distance_threshold: float = 3.0,
) -> tf.data.Dataset:
    """Creates a confmaps dataset with confmaps from all points.

    Args:
        ds_img_and_pts: A tf.data.Dataset that generates tuples of (image, points).
        edges: Array of shape (n_edges, 2) and dtype int where each row defines the
            (src_node_ind, dst_node_ind) that index into axis 1 of points to create the
            directed edges of the PAFs.
        output_scale: Relative scale (size) of the output PAF tensor.
        distance_threshold: Determines how far way from the edge lines that the PAF
            vectors should be defined (in full scale absolute image units). Increase
            this if the PAFs need to have increased support on the image (e.g., for
            thicker limbs).

    Returns:
        ds_pafs: A tf.data.Dataset that returns elements that are tuples of
        (image, pafs), where pafs contains the generated PAFs.
    """

    def gen_paf_fn(img, pts, ctr_pts=None):

        pts = tf.cast(pts, tf.float32)

        pafs = make_pafs(
            pts,
            edges,
            image_height=tf.shape(img)[0],
            image_width=tf.shape(img)[1],
            output_scale=output_scale,
            distance_threshold=distance_threshold,
        )

        return img, pafs

    ds_pafs = ds_img_and_pts.map(
        gen_paf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE
    )

    return ds_pafs
